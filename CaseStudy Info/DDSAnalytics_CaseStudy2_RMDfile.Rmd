---
#title: DDSAnalytics: Talent Management for a Changing World
## Presenters: Amy Markum, Andy Nguyen, Richard Hart, and Tom Gianelle - Predicting Employee Turnover
### date: "Spring 2019"

#### Load the necessary library packages prior to loading the data. 
#### Packages used include : 
####  -tidyquant
####  -readxl
####  -h2o
####  -lime
####  -ggplot2 

##### 1. Read in your data file.
```r
library(readxl)
hr_data_raw <- read_excel(path="CaseStudy2-singlesheet.xlsx")
```
##### 2. View your first 10 rows to ensure everything went as planned.
```r
hr_data_raw[1:10,] %>%
  knitr::kable(caption = "First 10 rows")
```
##### 3. Change your character data types to factors (needed by h20).
```r
hr_data <-hr_data_raw %>%
  mutate_if(is.character, as.factor) %>%
  select(Attrition, everything())
```
##### 4. Let's take a look at our modified set.
```r
glimpse(hr_data)
```
##### 5. Now let's model our attrition. NOTE: This package requires java (JDK 8), available from ORACLE. Also, turn off your output of progress bars, it's annoying
```r
h2o.init() # Initiate H20 JVM if you get an error, please install JAVA
h2o.no_progress()
```
##### 6. Change the dataset so that h20 can understand it and split the data into training, validation, and test sets (preferences at 70%, 15%, and 15%, respectively).
```r
hr_data_h2o <- as.h2o(hr_data)
split_h2o <- h2o.splitFrame(hr_data_h2o, c(0.7, 0.15), seed = 1234)
train_h2o <- h2o.assign(split_h2o[[1]], "train") # 70%
valid_h2o <- h2o.assign(split_h2o [[2]], "valid") # 15%
test_h2o <- h2o.assign(split_h2o[[3]], "test") # 15%
```
##### 7. Now we're ready to model. Set your target & feature names - the target is what we are trying to predict (Attrition) and features are the other columns used to model that prediction.
```r
y <- "Attrition"
X <- setdiff(names(train_h2o), y)
automl_models_h2o <- h2o.automl(
  x = x,
  y = y,
  training_frame = train_h2o,
  leaderboard_frame = valid_h2o,
  max_runtime_secs = 90)
```
##### 8. Your leader model is best in terms of accuracy for the validation set. Extract your leader model thus:
```r
automl_leader <- automl_models_h2o@leader
```
##### 9. Now predict for your test set (true test of performance)
```r
pred_h2o <- h2o.predict(object = automl_leader, newdata = test_h2o)
```
##### 10. Evaluate the leader model. We also reformat the test set to include predictions as a column so we have actual vs. prediction columns beside each other.
```r
test_performance <- test_h2o %>%
  tibble::as_tibble() %>%
  select(Attrition) %>%
  add_column(pred = as.vector(pred_h2o$predict)) %>%
  mutate_if(is.character, as.factor)
test_performance
```
##### 11. Get your table of results. The leader model wasn't perfect, but did a decent job of identifying employees who are likely to quit. NOTE: Logistic regression would not have performed as well in this instance)
```r
confusion_matrix <- test_performance %>%
  table()
confusion_matrix
```
##### 12. Next step is a binary classification analysis to understand how the model performed.
```r
tn <- confusion_matrix[1]
tp <- confusion_matrix[4]
fp <- confusion_matrix[3]
fn <- confusion_matrix[2]

accuracy <- (tp + tn) / (tp + tn + fp + fn)
misclassification_rate <- 1 - accuracy
recall <- tp / (tp + fn)
precision <- tp / (tp + fp)
null_error_rate <- tn / (tp + tn + fp + fn)

tibble(
  accuracy,
  misclassification_rate,
  recall,
  precision,
  null_error_rate
) %>%
  transpose()
```
##### Responses are: Accuracy - 0.872, Misclassification - 0.128, Recall - 0.620, Precision - 0.529, Error rate - 0.787

##### As you can see, our results show us at 87% accuracy, which sounds good until we see that if we just picked Attrition = NO, our accuracy is 79%. However, let's move on to precision and recall. Precision indicates how often the model accurately predicts "YES" and Recall indicates how often the model is correct when the actual value is "YES".

##### Something to keep in mind is that results need to be looked at within an HR management standpoint. It's important to not miss "at risk" employees, which means HR is interested in the Recall percentage. In layman's terms, with our model Recall at 62% that indicates if the organization loses 100 employees annually, HR can potentially target 62 of them for implementation of retention actions. That's a pretty good amount.

##### 13. So let's move on to determining the top causes of attrtion. This is where LIME comes in. NOTE: Lime needs to be tweaked in order to perform effectively with h20:
```r
class(automl_leader)
model_type.H2OBinomialModel <- function(x, ...) {return("classification")}
```
##### 14. Create a prediction model function
```r
# Setup lime::predict_model() function for h2o
predict_model.H2OBinomialModel <- function(x, newdata, type, ...) {
  # Function performs prediction and returns a dataframe with Response
  # x is h2o model
  # newdata is data frame
  # type is only setup for data frame
  
pred <- h2o.predict(x, as.h2o(newdata))
  # return probability
  return (as.data.frame(pred[,-1]))
}
```
##### 15. Test the prediction model function
```r
predict_model(x=automl_leader, newdata = as.data.frame(test_h2o[,-1]), type = "raw") %>%
  tibble::as_tibble()
  ```
##### 16. Create an explanation
```r
explainer <- lime::lime(
  as.data.frame(train_h2o[,-1]),
  model = automl_leader,
  bin_continuous = FALSE)
```
##### 17. Run your functions to return an explanation for attrition
#####   Using n_labels = 1 means we only care about a single class
#####   Setting n_features = 4 returns the top 4 features critical to each case
#####   Setting kernel_width = 0.5 increases the model value by shrinking localized evaluation
```r
explanation <- lime:: explain(
  as.data.frame(test_h2o[1:10, -1]),
  explainer = explainer,
  n_labels = 1,
  n_features = 4,
  kernel_width = 0.5)
```
##### 18. Plot your results to visualze the 10 cases from the test data. The top 4 features for each case are shown (note that they are not the same for each case). The green bars indicate support for model conclusion while the red bars are contradictory.
```r
plot_features(explanation) +
  labs(title= "HR Predictive Analytics: LIME Feature Importance Visualiztion",
       Subtitle = "Hold Out (test), Set, First 10 Cases Shown")
```
##### 19. Now let us turn our attention to the top 3 critical features of the feature importance plot:
##### 1) Training Time
##### 2) Job Role
##### 3) Overtime
```r
attrition_critical_features <- hr_data %>%
  tibble::as_tibble() %>%
  select(Attrition, TrainingTimesLastYear, JobRole, OverTime) %>%
  rowid_to_column(var = "Case")
attrition_critical_features
```
##### 20. Creating violin plots for these 3 features will help us to visualize. Still need to add these CAO 20190324
```r
ggplot2::geom_violin()
```